# WittgenLab ğŸ§ª

Un framework integral de evaluaciÃ³n para modelos de IA que proporciona una interfaz unificada para ejecutar mÃ©tricas, benchmarks y anÃ¡lisis.

## âœ¨ CaracterÃ­sticas Principales

- **ğŸ¯ API Unificada**: Interfaz Ãºnica para todas las necesidades de evaluaciÃ³n
- **ğŸ“Š MÃ©tricas Integrales**: MÃ©tricas n-gram, semÃ¡nticas, perplexity y personalizadas
- **ğŸ† Benchmarks EstÃ¡ndar**: GLUE, MMLU, HumanEval, TruthfulQA y mÃ¡s
- **ğŸ¤– LLM-as-a-Judge**: Soporte integrado para evaluaciÃ³n basada en LLM
- **ğŸ‘¥ EvaluaciÃ³n Humana**: Herramientas para anotaciÃ³n y crowdsourcing
- **ğŸ“ˆ AnÃ¡lisis y VisualizaciÃ³n**: AnÃ¡lisis de correlaciÃ³n, detecciÃ³n de sesgos y reportes
- **âš¡ FÃ¡cil de Usar**: API simple e intuitiva con configuraciÃ³n mÃ­nima
- **ğŸ”§ Extensible**: Sistema de plugins para mÃ©tricas y benchmarks personalizados

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
# InstalaciÃ³n bÃ¡sica
pip install wittgenlab

# Con todas las dependencias opcionales
pip install wittgenlab[full]

# InstalaciÃ³n para desarrollo
git clone https://github.com/yourusername/wittgenlab.git
cd wittgenlab
poetry install
```

### Uso BÃ¡sico

```python
from wittgenlab import EvalHub

# Inicializar el hub de evaluaciÃ³n
evaluator = EvalHub()

# Evaluar salidas de modelos con mÃºltiples mÃ©tricas
results = evaluator.evaluate(
    predictions=model_outputs,
    references=ground_truth,
    metrics=['bleu', 'bertscore', 'rouge'],
    task='summarization'
)

print(f"BLEU Score: {results.get_score('bleu'):.4f}")
print(f"BERTScore: {results.get_score('bertscore'):.4f}")
```

### BERTScore - EvaluaciÃ³n SemÃ¡ntica

```python
# BERTScore con configuraciones completas
results = evaluator.evaluate(
    predictions=predictions,
    references=references,
    metrics=['bertscore'],
    task='semantic_evaluation',
    return_full_scores=True,  # Obtener precisiÃ³n, recall, F1
    model_type="bert-base-multilingual-cased"
)

bertscore_result = results.get_score('bertscore')
print(f"PrecisiÃ³n: {bertscore_result['precision']:.4f}")
print(f"Recall: {bertscore_result['recall']:.4f}")
print(f"F1: {bertscore_result['f1']:.4f}")
```

### LLM-as-a-Judge

```python
# Juez con mÃºltiples modelos y criterios
judge_results = evaluator.judge(
    predictions=outputs,
    criteria=['accuracy', 'helpfulness', 'safety'],
    judge_models=['gpt-4', 'claude-3', 'gemini-pro'],
    consensus_method='majority_vote'
)

# Acceder a resultados por criterio
for criterion, stats in judge_results['summary'].items():
    print(f"{criterion}: {stats['mean_score']:.2f}/5")
```

### EvaluaciÃ³n de Benchmarks

```python
# Ejecutar benchmarks estandarizados
benchmark_results = evaluator.benchmark(
    model=my_model,
    benchmarks=['mmlu', 'hellaswag', 'truthfulqa'],
    few_shot=5
)

print(f"MMLU Score: {benchmark_results.get_score('mmlu'):.4f}")
```

### ComparaciÃ³n de Modelos

```python
# Comparar mÃºltiples modelos
models = {
    'gpt-4': gpt4_model,
    'claude-3': claude_model,
    'llama-2': llama_model
}

comparison = evaluator.compare_models(
    models=models,
    benchmarks=['mmlu', 'hellaswag'],
    few_shot=3
)

# Obtener rankings
ranking = comparison.get_overall_ranking()
print(f"Mejor modelo: {ranking[0][0]} con puntuaciÃ³n {ranking[0][1]:.4f}")
```

## ğŸ“ Estructura del Framework

```
wittgenlab/
â”œâ”€â”€ metrics/           # MÃ©tricas de evaluaciÃ³n
â”‚   â”œâ”€â”€ ngram/        # BLEU, ROUGE, METEOR, CIDEr
â”‚   â”œâ”€â”€ semantic/     # BERTScore, MoverScore, BLEURT
â”‚   â”œâ”€â”€ perplexity/   # MÃ©tricas basadas en perplejidad
â”‚   â””â”€â”€ custom/       # MÃ©tricas personalizadas
â”œâ”€â”€ benchmarks/       # Benchmarks estandarizados
â”‚   â”œâ”€â”€ glue/         # GLUE, SuperGLUE
â”‚   â”œâ”€â”€ knowledge/    # MMLU, ARC, HellaSwag
â”‚   â”œâ”€â”€ code/         # HumanEval, MBPP
â”‚   â”œâ”€â”€ safety/       # ToxiGen, TruthfulQA
â”‚   â””â”€â”€ multilingual/ # Benchmarks multilingÃ¼es
â”œâ”€â”€ judges/           # Implementaciones LLM-as-a-Judge
â”‚   â”œâ”€â”€ pairwise/     # Comparaciones pareadas
â”‚   â”œâ”€â”€ scoring/      # PuntuaciÃ³n absoluta
â”‚   â””â”€â”€ reasoning/    # Juicio con cadena de pensamiento
â”œâ”€â”€ human/            # Herramientas de evaluaciÃ³n humana
â”‚   â”œâ”€â”€ annotation/   # Interfaces de anotaciÃ³n
â”‚   â”œâ”€â”€ crowdsource/  # Integraciones de plataformas
â”‚   â””â”€â”€ agreement/    # Acuerdo entre anotadores
â””â”€â”€ analysis/         # AnÃ¡lisis y visualizaciÃ³n
    â”œâ”€â”€ correlation/  # AnÃ¡lisis de correlaciÃ³n
    â”œâ”€â”€ bias/         # DetecciÃ³n de sesgos
    â””â”€â”€ reports/      # GeneraciÃ³n de reportes
```

## ğŸ“Š MÃ©tricas Soportadas

### MÃ©tricas N-gram
- **BLEU**: Bilingual Evaluation Understudy
- **ROUGE**: Recall-Oriented Understudy for Gisting Evaluation
- **METEOR**: Metric for Evaluation of Translation with Explicit ORdering
- **CIDEr**: Consensus-based Image Description Evaluation

### MÃ©tricas SemÃ¡nticas
- **BERTScore**: Similitud semÃ¡ntica consciente del contexto
- **MoverScore**: Earth Mover's Distance para evaluaciÃ³n
- **BLEURT**: MÃ©trica de evaluaciÃ³n aprendida

### LLM-as-a-Judge
- **Criterios MÃºltiples**: accuracy, helpfulness, safety, quality, relevance, fluency
- **Modelos MÃºltiples**: GPT-4, Claude-3, Gemini-Pro, y mÃ¡s
- **MÃ©todos de Consenso**: majority_vote, average, weighted_average
- **Criterios Personalizados**: Define tus propios criterios de evaluaciÃ³n

### MÃ©tricas Personalizadas
- **Diversidad**: Diversidad lÃ©xica y semÃ¡ntica
- **Coherencia**: EvaluaciÃ³n de coherencia textual
- **Fluidez**: EvaluaciÃ³n de fluidez del lenguaje

## ğŸ† Benchmarks Soportados

### Conocimiento y Razonamiento
- **MMLU**: Massive Multitask Language Understanding
- **ARC**: AI2 Reasoning Challenge
- **HellaSwag**: Razonamiento de sentido comÃºn
- **WinoGrande**: Winograd Schema Challenge

### GeneraciÃ³n de CÃ³digo
- **HumanEval**: GeneraciÃ³n de cÃ³digo Python
- **MBPP**: Mostly Basic Python Problems

### Seguridad y AlineaciÃ³n
- **ToxiGen**: DetecciÃ³n de toxicidad
- **TruthfulQA**: EvaluaciÃ³n de veracidad
- **DetecciÃ³n de Sesgos**: Varias suites de evaluaciÃ³n de sesgos

## âš™ï¸ ConfiguraciÃ³n

Crear configuraciones personalizadas para tus necesidades de evaluaciÃ³n:

```python
from wittgenlab import EvalConfig

config = EvalConfig(
    batch_size=32,
    use_cache=True,
    log_level="INFO",
    output_dir="./eval_results"
)

# Configurar mÃ©tricas especÃ­ficas
config.set_metric_config('bleu', {
    'max_order': 4,
    'smooth': True
})

# Configurar BERTScore
config.set_metric_config('bertscore', {
    'model_type': 'bert-base-multilingual-cased',
    'return_full_scores': True
})

evaluator = EvalHub(config=config)
```

## ğŸ¤– LLM-as-a-Judge Avanzado

### Uso BÃ¡sico

```python
from wittgenlab.judges import create_judge

# Crear un juez individual
judge = create_judge(model_name="gpt-4o-mini")

result = judge.evaluate(
    prediction="Tu texto a evaluar",
    criterion="accuracy",
    context="Contexto opcional"
)

print(f"PuntuaciÃ³n: {result.score}/5")
print(f"JustificaciÃ³n: {result.justification}")
```

### MÃºltiples Modelos con Consenso

```python
# Configurar mÃºltiples modelos
judge_results = evaluator.judge(
    predictions=["Texto 1", "Texto 2"],
    criteria=['accuracy', 'helpfulness', 'safety'],
    judge_models=['gpt-4', 'claude-3', 'gemini-pro'],
    consensus_method='majority_vote'
)

# AnÃ¡lisis de resultados
for criterion, stats in judge_results['summary'].items():
    print(f"{criterion}: {stats['mean_score']:.2f}/5")
```

### Criterios Personalizados

```python
# Definir criterios personalizados
custom_result = judge.evaluate(
    prediction=code_snippet,
    criterion="code_quality_and_efficiency",
    context="FunciÃ³n para calcular Fibonacci"
)
```

## ğŸ“ˆ AnÃ¡lisis y Reportes

Generar reportes integrales de evaluaciÃ³n:

```python
# Generar reporte HTML
report_path = evaluator.generate_report(
    results=benchmark_results,
    format="html",
    output_path="evaluation_report.html"
)

# Analizar correlaciones entre mÃ©tricas
from wittgenlab.analysis import CorrelationAnalyzer

analyzer = CorrelationAnalyzer()
correlations = analyzer.analyze(results)
```

## ğŸ”§ Extensibilidad del Framework

### Agregar MÃ©tricas Personalizadas

```python
from wittgenlab.metrics.base import ReferenceBasedMetric

class MyCustomMetric(ReferenceBasedMetric):
    def _compute_score(self, predictions, references):
        # Tu implementaciÃ³n de mÃ©trica
        return score

# Registrar la mÃ©trica
evaluator.metrics_registry.register_metric("my_metric", MyCustomMetric)
```

### Agregar Benchmarks Personalizados

```python
from wittgenlab.benchmarks.base import BaseBenchmark

class MyBenchmark(BaseBenchmark):
    def load_data(self):
        # Cargar tus datos de benchmark
        return data
    
    def evaluate(self, model):
        # LÃ³gica de evaluaciÃ³n
        return results

# Registrar el benchmark
evaluator.benchmarks_registry.register_benchmark("my_benchmark", MyBenchmark)
```

### Implementar Jueces Personalizados

```python
from wittgenlab.judges.base import BaseJudge, JudgeResult

class CustomJudge(BaseJudge):
    def _initialize_model(self):
        # Inicializar tu modelo personalizado
        pass
    
    def evaluate(self, prediction, reference=None, criterion="quality", **kwargs):
        # Tu lÃ³gica de evaluaciÃ³n personalizada
        return JudgeResult(
            score=score,
            justification="Tu justificaciÃ³n",
            criterion=criterion,
            model_name=self.model_name
        )
```

## ğŸš€ Ejemplos Completos

### Ejemplo 1: EvaluaciÃ³n de ResÃºmenes

```python
from wittgenlab import EvalHub

evaluator = EvalHub()

# Datos de ejemplo
summaries = ["Resumen generado por modelo...", ...]
references = ["Resumen de referencia...", ...]

# EvaluaciÃ³n combinada
traditional_results = evaluator.evaluate(
    predictions=summaries,
    references=references,
    metrics=['bleu', 'rouge', 'bertscore'],
    task='summarization'
)

llm_results = evaluator.judge(
    predictions=summaries,
    criteria=['accuracy', 'coherence', 'conciseness'],
    judge_models=['gpt-4'],
    references=references
)

print("MÃ©tricas AutomÃ¡ticas:")
for metric, score in traditional_results.get_all_scores().items():
    print(f"  {metric}: {score:.4f}")

print("EvaluaciÃ³n LLM:")
for criterion, stats in llm_results['summary'].items():
    print(f"  {criterion}: {stats['mean_score']:.2f}/5")
```

### Ejemplo 2: ComparaciÃ³n de Chatbots

```python
# Comparar respuestas de diferentes chatbots
chatbot_responses = {
    "ChatGPT": ["Respuesta 1", "Respuesta 2"],
    "Claude": ["Respuesta 1", "Respuesta 2"],
    "Gemini": ["Respuesta 1", "Respuesta 2"]
}

for bot_name, responses in chatbot_responses.items():
    results = evaluator.judge(
        predictions=responses,
        criteria=['helpfulness', 'safety', 'accuracy'],
        judge_models=['gpt-4', 'claude-3'],
        consensus_method='average'
    )
    
    print(f"\n{bot_name} Results:")
    for criterion, stats in results['summary'].items():
        print(f"  {criterion}: {stats['mean_score']:.2f}/5")
```

## ğŸ“š DocumentaciÃ³n

Para documentaciÃ³n detallada, ejemplos y referencia de API, visita:
- [DocumentaciÃ³n Completa](https://wittgenlab.readthedocs.io)
- [Referencia de API](https://wittgenlab.readthedocs.io/api)
- [Ejemplos](https://github.com/yourusername/wittgenlab/tree/main/examples)

## ğŸ® Ejecutar la DemostraciÃ³n

```bash
# Ejecutar ejemplo bÃ¡sico
python examples/basic_usage.py

# Ejecutar demostraciÃ³n avanzada
python examples/advanced_usage.py

# Ejecutar demostraciÃ³n completa
python examples/complete_demo.py
```

## ğŸ¤ Contribuir

Â¡Damos la bienvenida a las contribuciones! Consulta nuestra [GuÃ­a de ContribuciÃ³n](CONTRIBUTING.md) para mÃ¡s detalles.

1. Haz fork del repositorio
2. Crea una rama de feature
3. Realiza tus cambios
4. Agrega tests
5. EnvÃ­a un pull request

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - consulta el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ™ Agradecimientos

- Construido sobre excelentes librerÃ­as como HuggingFace Transformers, SacreBLEU, BERTScore y LangChain
- Inspirado por frameworks de evaluaciÃ³n de la comunidad de investigaciÃ³n
- Gracias a todos los contribuidores y usuarios del framework

## ğŸ“ Contacto

- **Autor**: Robert Gomez
- **Email**: robertgomez.datascience@gmail.com
- **GitHub**: [@Robert-Gomez-AI](https://github.com/Robert-Gomez-AI)

---

â­ **Â¡Dale estrella a este repositorio si te resulta Ãºtil!** â­
